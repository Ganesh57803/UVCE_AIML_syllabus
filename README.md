# BANGALORE UNIVERSITY

Department of Computer Science and Engineering, UVCE, Bengaluru.
Scheme and Syllabus - NEP-2022

## ARTIFICIAL NEURAL NETWORK-THEORY

**Course Code:** 21AIPC603
**Category:** Professional Core Courses
**Scheme** | **L** | **T** | **P** | **SS** | **Credits** | **Semester**
------- | -------- | -------- | -------- | -------- | -------- | --------
&nbsp; | 02 | 02 | 00 | 00 | 03 | VI
**CIE Marks:** 50 **SEE Marks:** 50 **Total Max. Marks:** 100
**Duration of SEE:** 03 Hours

## COURSE OBJECTIVES

The course will enable the students to:

1. Know the qualitative terms like neural networks, their properties, compositions and how they relate to artificial Intelligence.
2. Know about the overview of many facts of the leaming process and its statistical properties
3. Understand the simplest class of neural network part, LMS algorithm and the perception convergence theorem.
4. Know about exhaustive treatment of multilayer perceptions trained with the back-propagation algorithm.
5. Understand about mapping models and also self-organized learning like competitive learning to construct a computational maps known as self-organized maps.
6. Exposed to dynamic systems stability aspects and the concepts of attraction and neuro dynamic models are discussed.

## UNIT 1

**10 Hours**

**Introduction:** What is a neural network? Human Brain, Models of a Neuron, Neural networks viewed as Directed Graphs, Network Architectures, Knowledge Representation, Artificial Imelligence and Neural Networks.
**Learning Process 1:** Error Correction learning, Memory based learning, Hebbian learing.

## UNIT 2

**10 Hours**

**Learning Process 2:** Competitive, Boltzmann leaming, Credit Assignment Problem, Memory, Adaption, Statistical nature of the learning process.
**Single Layer Perceptrons** Adaptive filtering problem, Unconstrained Organization Techniques, Linear least square filters, least mean square algorithm, learning curves, Learning rate annealing techniques, perception-convergence theorem, Relation between perception and Bayes classifier for a Gaussian Environment.

## UNIT 3

**10 Hours**

**Multilayer Perceptron:** Back propagation algorithm XOR problem, Heuristics, Output representation and decision rule, Computer experiment, feature detection.
 **Back Propagation -** back propagation and differentiation, Hessian matrix , Newton's method, Conjugate gradient, Quasi-Newton method, Levenberg-Marquardt algorithm, Learning rate adaptation, Momentum, Nesterov Accelerated Gradient, Batch Normalization, Regularization Techniques, Early Stopping, Dropout, L1 and L2 Regularization, Cross-Entropy Loss, Softmax Output, Gradient Computation, Gradient Descent with Momentum, Gradient Descent with Nesterov Acceleration, Gradient Descent with Batch Normalization, Gradient Descent with Regularization.

## UNIT 4

**10 Hours**

**Self-Organizing Maps and Applications:** Introduction to Self-Organizing Maps, Competitive Learning, Kohonen's SOM, SOM Algorithm, SOM Applications, Image Compression, Image Segmentation, Image Recognition, Time Series Prediction, Clustering, Dimensionality Reduction.

## TEXTBOOKS

1. **Haykin, S. S. (2009). Neural Networks and Learning Machines.** Prentice Hall.
2. **Bishop, C. M. (2006). Pattern Recognition and Machine Learning.** Springer.
3. **Rojas, R. (1996). Neural Networks: A Systematic Introduction.** Springer.

## ONLINE RESOURCES

1. **YouTube:** [Artificial Neural Networks Tutorial for Beginners](https://www.youtube.com/watch?v=IHZwWFHWa-w)
2. **YouTube:** [Neural Networks Tutorial by 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1O8Q3LSpQiQyD7X_t_)
3. **Kaggle:** [Neural Networks Tutorial](https://www.kaggle.com/learn/neural-networks)

Note: The YouTube links provided are for reference purposes only and are not affiliated with the course or the university.